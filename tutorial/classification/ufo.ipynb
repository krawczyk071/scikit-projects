{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://goodboychan.github.io/python/datacamp/machine_learning/2020/07/10/02-Preprocessing-Putting-it-all-together.html\n",
    "# https://www.kaggle.com/code/suhailsh7/data-preprocessing-ufo-sighting-month-classify/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>type</th>\n",
       "      <th>seconds</th>\n",
       "      <th>length_of_time</th>\n",
       "      <th>desc</th>\n",
       "      <th>recorded</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/3/2011 19:21</td>\n",
       "      <td>woodville</td>\n",
       "      <td>wi</td>\n",
       "      <td>us</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1209600.0</td>\n",
       "      <td>2 weeks</td>\n",
       "      <td>Red blinking objects similar to airplanes or s...</td>\n",
       "      <td>12/12/2011</td>\n",
       "      <td>44.9530556</td>\n",
       "      <td>-92.291111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/3/2004 19:05</td>\n",
       "      <td>cleveland</td>\n",
       "      <td>oh</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30sec.</td>\n",
       "      <td>Many fighter jets flying towards UFO</td>\n",
       "      <td>10/27/2004</td>\n",
       "      <td>41.4994444</td>\n",
       "      <td>-81.695556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/25/2009 21:00</td>\n",
       "      <td>coon rapids</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>cigar</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green&amp;#44 red&amp;#44 and blue pulses of light tha...</td>\n",
       "      <td>12/12/2009</td>\n",
       "      <td>45.1200000</td>\n",
       "      <td>-93.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/21/2002 05:45</td>\n",
       "      <td>clemmons</td>\n",
       "      <td>nc</td>\n",
       "      <td>us</td>\n",
       "      <td>triangle</td>\n",
       "      <td>300.0</td>\n",
       "      <td>about 5 minutes</td>\n",
       "      <td>It was a large&amp;#44 triangular shaped flying ob...</td>\n",
       "      <td>12/23/2002</td>\n",
       "      <td>36.0213889</td>\n",
       "      <td>-80.382222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/19/2010 12:55</td>\n",
       "      <td>calgary (canada)</td>\n",
       "      <td>ab</td>\n",
       "      <td>ca</td>\n",
       "      <td>oval</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>A white spinning disc in the shape of an oval.</td>\n",
       "      <td>8/24/2010</td>\n",
       "      <td>51.083333</td>\n",
       "      <td>-114.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date              city state country      type    seconds  \\\n",
       "0   11/3/2011 19:21         woodville    wi      us   unknown  1209600.0   \n",
       "1   10/3/2004 19:05         cleveland    oh      us    circle       30.0   \n",
       "2   9/25/2009 21:00       coon rapids    mn      us     cigar        0.0   \n",
       "3  11/21/2002 05:45          clemmons    nc      us  triangle      300.0   \n",
       "4   8/19/2010 12:55  calgary (canada)    ab      ca      oval        0.0   \n",
       "\n",
       "    length_of_time                                               desc  \\\n",
       "0          2 weeks  Red blinking objects similar to airplanes or s...   \n",
       "1           30sec.               Many fighter jets flying towards UFO   \n",
       "2              NaN  Green&#44 red&#44 and blue pulses of light tha...   \n",
       "3  about 5 minutes  It was a large&#44 triangular shaped flying ob...   \n",
       "4                2     A white spinning disc in the shape of an oval.   \n",
       "\n",
       "     recorded         lat        long  \n",
       "0  12/12/2011  44.9530556  -92.291111  \n",
       "1  10/27/2004  41.4994444  -81.695556  \n",
       "2  12/12/2009  45.1200000  -93.287500  \n",
       "3  12/23/2002  36.0213889  -80.382222  \n",
       "4   8/24/2010   51.083333 -114.083333  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufo = pd.read_csv('./ufo_sightings_large.csv')\n",
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date               object\n",
      "city               object\n",
      "state              object\n",
      "country            object\n",
      "type               object\n",
      "seconds           float64\n",
      "length_of_time     object\n",
      "desc               object\n",
      "recorded           object\n",
      "lat                object\n",
      "long              float64\n",
      "dtype: object\n",
      "seconds           float64\n",
      "date       datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(ufo.dtypes)\n",
    "\n",
    "# Change the type of seconds to float\n",
    "ufo['seconds'] = ufo['seconds'].astype(float)\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo['date'] = pd.to_datetime(ufo['date'])\n",
    "\n",
    "# Check the column types\n",
    "print(ufo[['seconds', 'date']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_of_time    143\n",
      "state             419\n",
      "type              159\n",
      "dtype: int64\n",
      "(4283, 11)\n"
     ]
    }
   ],
   "source": [
    "print(ufo[['length_of_time', 'state', 'type']].isnull().sum())\n",
    "\n",
    "# Keep only rows where length_of_time, state, and type are not null\n",
    "ufo_no_missing = ufo[ufo['length_of_time'].notnull() &\n",
    "                     ufo['state'].notnull() & \n",
    "                     ufo['type'].notnull()]\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print(ufo_no_missing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    length_of_time  minutes\n",
      "0  about 5 minutes      NaN\n",
      "1       10 minutes     10.0\n",
      "2        2 minutes      2.0\n",
      "3        2 minutes      2.0\n",
      "4        5 minutes      5.0\n",
      "5       10 minutes     10.0\n",
      "6        5 minutes      5.0\n",
      "7        5 minutes      5.0\n",
      "8        5 minutes      5.0\n",
      "9          1minute      1.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "ufo = pd.read_csv('./ufo_sample.csv')\n",
    "\n",
    "# Change the type of seconds to float\n",
    "ufo['seconds'] = ufo['seconds'].astype(float)\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo['date'] = pd.to_datetime(ufo['date'])\n",
    "\n",
    "def return_minutes(time_string):\n",
    "    # Use \\d+ to grab digits\n",
    "    pattern = re.compile(r'\\d+')\n",
    "\n",
    "    # Use match on th epattern and column\n",
    "    num = re.match(pattern, time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "    \n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo['minutes'] = ufo['length_of_time'].apply(lambda row: return_minutes(row))\n",
    "\n",
    "# Take a look at the head of both of th ecolumns\n",
    "print(ufo[['length_of_time', 'minutes']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds    424087.417474\n",
      "minutes       117.546372\n",
      "dtype: float64\n",
      "1.1223923881183004\n"
     ]
    }
   ],
   "source": [
    "print(ufo[['seconds', 'minutes']].var())\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo['seconds_log'] = np.log(ufo['seconds'])\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo['seconds_log'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "ufo['country_enc'] = ufo['country'].apply(lambda x: 1 if x == 'us' else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo['type'].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo['type'])\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n",
      "                 date  month  year\n",
      "0 2011-11-03 19:21:00     11  2011\n",
      "1 2004-10-03 19:05:00     10  2004\n",
      "2 2009-09-25 21:00:00      9  2009\n",
      "3 2002-11-21 05:45:00     11  2002\n",
      "4 2010-08-19 12:55:00      8  2010\n"
     ]
    }
   ],
   "source": [
    "print(ufo['date'].dtypes)\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo['month'] = ufo['date'].apply(lambda date: date.month)\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo['year'] = ufo['date'].apply(lambda date: date.year)\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "print(ufo[['date', 'month', 'year']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    It was a large&#44 triangular shaped flying ob...\n",
      "1    Dancing lights that would fly around and then ...\n",
      "2    Brilliant orange light or chinese lantern at o...\n",
      "3    Bright red light moving north to north west fr...\n",
      "4    North-east moving south-west. First 7 or so li...\n",
      "Name: desc, dtype: object\n",
      "(1866, 3422)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Take a look at the head of the desc field\n",
    "print(ufo['desc'].head())\n",
    "\n",
    "# Create the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# Use vec's fit_transform method on the desc field\n",
    "desc_tfidf = vec.fit_transform(ufo['desc'])\n",
    "\n",
    "# Look at the number of columns this creates\n",
    "print(desc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Let's transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "        # here we'll call the function from the previous exercise, \n",
    "        # and extend the list we're creating\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_csv = pd.read_csv('./vocab_ufo.csv', index_col=0).to_dict()\n",
    "vocab = vocab_csv['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              seconds  seconds_log   minutes\n",
      "seconds      1.000000     0.853371  0.980341\n",
      "seconds_log  0.853371     1.000000  0.824493\n",
      "minutes      0.980341     0.824493  1.000000\n"
     ]
    }
   ],
   "source": [
    "print(ufo[['seconds', 'seconds_log', 'minutes']].corr())\n",
    "\n",
    "# Make a list of features to drop\n",
    "to_drop = ['city', 'country', 'date', 'desc', 'lat', \n",
    "           'length_of_time', 'seconds', 'minutes', 'long', 'state', 'recorded']\n",
    "\n",
    "# Drop those features\n",
    "ufo_dropped = ufo.drop(to_drop, axis=1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, top_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone',\n",
      "       'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash',\n",
      "       'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',\n",
      "       'teardrop', 'triangle', 'unknown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = ufo_dropped.drop(['type', 'country_enc'], axis=1)\n",
    "y = ufo_dropped['country_enc']\n",
    "\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Flags' object has no attribute 'c_contiguous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m knn\u001b[39m.\u001b[39mfit(train_X, train_y)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Print the score of knn on the test sets\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mprint\u001b[39m(knn\u001b[39m.\u001b[39;49mscore(test_X, test_y))\n",
      "File \u001b[1;32mc:\\Users\\krawc\\OneDrive\\Documents\\code\\pythons\\scikit-projects\\.venv\\Lib\\site-packages\\sklearn\\base.py:705\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m--> 705\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\krawc\\OneDrive\\Documents\\code\\pythons\\scikit-projects\\.venv\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:246\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    244\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_fit_method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 246\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbrute\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m ArgKminClassMode\u001b[39m.\u001b[39;49mis_usable_for(\n\u001b[0;32m    247\u001b[0m         X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetric\n\u001b[0;32m    248\u001b[0m     ):\n\u001b[0;32m    249\u001b[0m         probabilities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    250\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs_2d_:\n",
      "File \u001b[1;32mc:\\Users\\krawc\\OneDrive\\Documents\\code\\pythons\\scikit-projects\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:471\u001b[0m, in \u001b[0;36mArgKminClassMode.is_usable_for\u001b[1;34m(cls, X, Y, metric)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    449\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_usable_for\u001b[39m(\u001b[39mcls\u001b[39m, X, Y, metric) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m    450\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return True if the dispatcher can be used for the given parameters.\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \n\u001b[0;32m    452\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[39m    True if the PairwiseDistancesReduction can be used, else False.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 471\u001b[0m         ArgKmin\u001b[39m.\u001b[39;49mis_usable_for(X, Y, metric)\n\u001b[0;32m    472\u001b[0m         \u001b[39m# TODO: Support CSR matrices.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(X)\n\u001b[0;32m    474\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(Y)\n\u001b[0;32m    475\u001b[0m         \u001b[39m# TODO: implement Euclidean specialization with GEMM.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m         \u001b[39mand\u001b[39;00m metric \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39meuclidean\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msqeuclidean\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    477\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\krawc\\OneDrive\\Documents\\code\\pythons\\scikit-projects\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:115\u001b[0m, in \u001b[0;36mBaseDistancesReductionDispatcher.is_usable_for\u001b[1;34m(cls, X, Y, metric)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_valid_sparse_matrix\u001b[39m(X):\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    103\u001b[0m         isspmatrix_csr(X)\n\u001b[0;32m    104\u001b[0m         \u001b[39mand\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m         X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mint32\n\u001b[0;32m    111\u001b[0m     )\n\u001b[0;32m    113\u001b[0m is_usable \u001b[39m=\u001b[39m (\n\u001b[0;32m    114\u001b[0m     get_config()\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39menable_cython_pairwise_dist\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mand\u001b[39;00m (is_numpy_c_ordered(X) \u001b[39mor\u001b[39;00m is_valid_sparse_matrix(X))\n\u001b[0;32m    116\u001b[0m     \u001b[39mand\u001b[39;00m (is_numpy_c_ordered(Y) \u001b[39mor\u001b[39;00m is_valid_sparse_matrix(Y))\n\u001b[0;32m    117\u001b[0m     \u001b[39mand\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m Y\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    118\u001b[0m     \u001b[39mand\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39min\u001b[39;00m (np\u001b[39m.\u001b[39mfloat32, np\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m    119\u001b[0m     \u001b[39mand\u001b[39;00m metric \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mvalid_metrics()\n\u001b[0;32m    120\u001b[0m )\n\u001b[0;32m    122\u001b[0m \u001b[39mreturn\u001b[39;00m is_usable\n",
      "File \u001b[1;32mc:\\Users\\krawc\\OneDrive\\Documents\\code\\pythons\\scikit-projects\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:99\u001b[0m, in \u001b[0;36mBaseDistancesReductionDispatcher.is_usable_for.<locals>.is_numpy_c_ordered\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_numpy_c_ordered\u001b[39m(X):\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mflags\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m X\u001b[39m.\u001b[39;49mflags\u001b[39m.\u001b[39;49mc_contiguous\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Flags' object has no attribute 'c_contiguous'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y)\n",
    "\n",
    "# Fit knn to the training sets\n",
    "knn.fit(train_X, train_y)\n",
    "\n",
    "# Print the score of knn on the test sets\n",
    "print(knn.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14346895074946467\n"
     ]
    }
   ],
   "source": [
    "y = ufo_dropped['type']\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
    "\n",
    "# Fit nb to the training sets\n",
    "nb.fit(train_X, train_y)\n",
    "\n",
    "# Print the score of nb on the test sets\n",
    "print(nb.score(test_X, test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
